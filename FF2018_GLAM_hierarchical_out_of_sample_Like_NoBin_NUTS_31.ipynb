{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # from random.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Hierarchical GLAM estimation and out of sample prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.568396</td>\n",
       "      <td>0.431604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3371</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.553191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7466</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.532352</td>\n",
       "      <td>0.467648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1889</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.529736</td>\n",
       "      <td>0.470264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  trial  choice    rt  item_value_0  item_value_1    gaze_0  \\\n",
       "0        0      0       0  2009          1.10          0.95  0.568396   \n",
       "1        0      1       0  3371          2.00          1.70  0.762332   \n",
       "2        0      2       1  1700          1.10          2.30  0.446809   \n",
       "3        0      3       1  7466          1.25          1.40  0.532352   \n",
       "4        0      4       1  1889          2.00          2.30  0.529736   \n",
       "\n",
       "     gaze_1  \n",
       "0  0.431604  \n",
       "1  0.237668  \n",
       "2  0.553191  \n",
       "3  0.467648  \n",
       "4  0.470264  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "sufix = '_Like_NoBin_NUTS_31'\n",
    "data = pd.read_csv('data/FF2018_data/GlamDataFF2018_Like_NoBin_31.csv')\n",
    "# Subset only necessary columns\n",
    "data = data[['subject', 'trial', 'choice', 'rt',\n",
    "         'item_value_0', 'item_value_1',\n",
    "         'gaze_0', 'gaze_1']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.subject.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into training (1860 trials) and test (1860 trials) sets...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for subject in data.subject.unique():\n",
    "    subject_data = data[data['subject'] == subject].copy().reset_index(drop=True)\n",
    "    n_trials = len(subject_data)\n",
    "    \n",
    "    subject_train = subject_data.iloc[np.arange(0, n_trials, 2)].copy()\n",
    "    subject_test = subject_data.iloc[np.arange(1, n_trials, 2)].copy()\n",
    "\n",
    "    test_data = pd.concat([test_data, subject_test])\n",
    "    train_data = pd.concat([train_data, subject_train])\n",
    "\n",
    "test_data.to_csv(str('data/FF2018_data/GlamDataFF2018_preprocessed_test'+sufix+'.csv'))\n",
    "train_data.to_csv(str('data/FF2018_data/GlamDataFF2018_preprocessed_train'+sufix+'.csv'))\n",
    "\n",
    "print('Split data into training ({} trials) and test ({} trials) sets...'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical GLAM estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. full GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting full GLAM hierarchically...\n",
      "Generating hierarchical model for 31 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 model(s) using NUTS...\n",
      "  Fitting model 1 of 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [tau, tau_sd, tau_mu, SNR, SNR_sd, SNR_mu, gamma, gamma_sd, gamma_mu, v, v_sd, v_mu]\n",
      "Sampling 2 chains:   0%|          | 0/6000 [00:00<?, ?draws/s]/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Sampling 2 chains: 100%|██████████| 6000/6000 [1:14:46<00:00,  1.69s/draws]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/!\\ Automatically setting parameter precision...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNR</th>\n",
       "      <th>SNR_mu</th>\n",
       "      <th>SNR_sd</th>\n",
       "      <th>b</th>\n",
       "      <th>gamma</th>\n",
       "      <th>gamma_mu</th>\n",
       "      <th>gamma_sd</th>\n",
       "      <th>p_error</th>\n",
       "      <th>s</th>\n",
       "      <th>t0</th>\n",
       "      <th>tau</th>\n",
       "      <th>tau_mu</th>\n",
       "      <th>tau_sd</th>\n",
       "      <th>v</th>\n",
       "      <th>v_mu</th>\n",
       "      <th>v_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151.23</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137.71</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141.48</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157.60</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.65</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.58</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>113.65</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>163.65</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150.49</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>175.26</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>137.66</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.77</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>139.86</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>147.06</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>165.72</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>191.67</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>149.37</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.44</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150.47</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.53</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>152.69</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.87</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>102.32</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.56</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>114.35</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>77.01</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160.24</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190.84</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>108.44</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.41</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>175.71</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.98</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>195.32</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>138.12</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>123.71</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>182.16</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>203.52</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>169.70</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>185.39</td>\n",
       "      <td>164.29</td>\n",
       "      <td>35.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SNR  SNR_mu  SNR_sd    b  gamma  gamma_mu  gamma_sd  p_error         s  \\\n",
       "0   151.23  164.29   35.54  1.0  -0.11     -0.26      0.38     0.05  0.007843   \n",
       "1   137.71  164.29   35.54  1.0   0.29     -0.26      0.38     0.05  0.010344   \n",
       "2   141.48  164.29   35.54  1.0  -0.00     -0.26      0.38     0.05  0.008574   \n",
       "3   157.60  164.29   35.54  1.0  -0.10     -0.26      0.38     0.05  0.007178   \n",
       "4    96.58  164.29   35.54  1.0  -0.37     -0.26      0.38     0.05  0.007063   \n",
       "5   113.65  164.29   35.54  1.0   0.15     -0.26      0.38     0.05  0.008615   \n",
       "6   163.65  164.29   35.54  1.0  -0.00     -0.26      0.38     0.05  0.010656   \n",
       "7   150.49  164.29   35.54  1.0  -0.08     -0.26      0.38     0.05  0.007405   \n",
       "8   175.26  164.29   35.54  1.0  -0.71     -0.26      0.38     0.05  0.009622   \n",
       "9   137.66  164.29   35.54  1.0  -0.08     -0.26      0.38     0.05  0.007104   \n",
       "10  139.86  164.29   35.54  1.0  -0.07     -0.26      0.38     0.05  0.006689   \n",
       "11  147.06  164.29   35.54  1.0  -0.29     -0.26      0.38     0.05  0.006778   \n",
       "12  165.72  164.29   35.54  1.0  -0.10     -0.26      0.38     0.05  0.007923   \n",
       "13  191.67  164.29   35.54  1.0   0.15     -0.26      0.38     0.05  0.007864   \n",
       "14  149.37  164.29   35.54  1.0   0.42     -0.26      0.38     0.05  0.007737   \n",
       "15  150.47  164.29   35.54  1.0  -0.57     -0.26      0.38     0.05  0.007620   \n",
       "16  152.69  164.29   35.54  1.0   0.03     -0.26      0.38     0.05  0.008576   \n",
       "17  102.32  164.29   35.54  1.0  -0.38     -0.26      0.38     0.05  0.006303   \n",
       "18  114.35  164.29   35.54  1.0  -0.71     -0.26      0.38     0.05  0.007595   \n",
       "19   77.01  164.29   35.54  1.0  -0.72     -0.26      0.38     0.05  0.005931   \n",
       "20  160.24  164.29   35.54  1.0  -0.38     -0.26      0.38     0.05  0.007985   \n",
       "21  190.84  164.29   35.54  1.0  -0.17     -0.26      0.38     0.05  0.007118   \n",
       "22  108.44  164.29   35.54  1.0  -0.15     -0.26      0.38     0.05  0.008825   \n",
       "23  175.71  164.29   35.54  1.0  -0.40     -0.26      0.38     0.05  0.007353   \n",
       "24  195.32  164.29   35.54  1.0   0.07     -0.26      0.38     0.05  0.007485   \n",
       "25  138.12  164.29   35.54  1.0  -0.31     -0.26      0.38     0.05  0.009648   \n",
       "26  123.71  164.29   35.54  1.0  -0.79     -0.26      0.38     0.05  0.007309   \n",
       "27  182.16  164.29   35.54  1.0  -0.32     -0.26      0.38     0.05  0.010526   \n",
       "28  203.52  164.29   35.54  1.0  -0.75     -0.26      0.38     0.05  0.006983   \n",
       "29  169.70  164.29   35.54  1.0  -0.63     -0.26      0.38     0.05  0.007855   \n",
       "30  185.39  164.29   35.54  1.0  -0.37     -0.26      0.38     0.05  0.007719   \n",
       "\n",
       "     t0   tau  tau_mu  tau_sd         v      v_mu      v_sd  \n",
       "0   0.0  3.74    3.34    1.14  0.000047  0.000052  0.000013  \n",
       "1   0.0  4.30    3.34    1.14  0.000072  0.000052  0.000013  \n",
       "2   0.0  2.59    3.34    1.14  0.000058  0.000052  0.000013  \n",
       "3   0.0  4.65    3.34    1.14  0.000043  0.000052  0.000013  \n",
       "4   0.0  4.22    3.34    1.14  0.000080  0.000052  0.000013  \n",
       "5   0.0  2.80    3.34    1.14  0.000059  0.000052  0.000013  \n",
       "6   0.0  3.55    3.34    1.14  0.000069  0.000052  0.000013  \n",
       "7   0.0  3.29    3.34    1.14  0.000045  0.000052  0.000013  \n",
       "8   0.0  1.24    3.34    1.14  0.000053  0.000052  0.000013  \n",
       "9   0.0  4.77    3.34    1.14  0.000048  0.000052  0.000013  \n",
       "10  0.0  2.09    3.34    1.14  0.000046  0.000052  0.000013  \n",
       "11  0.0  2.97    3.34    1.14  0.000050  0.000052  0.000013  \n",
       "12  0.0  3.83    3.34    1.14  0.000047  0.000052  0.000013  \n",
       "13  0.0  2.52    3.34    1.14  0.000043  0.000052  0.000013  \n",
       "14  0.0  2.44    3.34    1.14  0.000045  0.000052  0.000013  \n",
       "15  0.0  4.53    3.34    1.14  0.000052  0.000052  0.000013  \n",
       "16  0.0  4.87    3.34    1.14  0.000055  0.000052  0.000013  \n",
       "17  0.0  4.56    3.34    1.14  0.000055  0.000052  0.000013  \n",
       "18  0.0  3.05    3.34    1.14  0.000058  0.000052  0.000013  \n",
       "19  0.0  3.59    3.34    1.14  0.000073  0.000052  0.000013  \n",
       "20  0.0  3.02    3.34    1.14  0.000044  0.000052  0.000013  \n",
       "21  0.0  1.36    3.34    1.14  0.000035  0.000052  0.000013  \n",
       "22  0.0  4.41    3.34    1.14  0.000079  0.000052  0.000013  \n",
       "23  0.0  4.98    3.34    1.14  0.000042  0.000052  0.000013  \n",
       "24  0.0  3.77    3.34    1.14  0.000034  0.000052  0.000013  \n",
       "25  0.0  2.39    3.34    1.14  0.000056  0.000052  0.000013  \n",
       "26  0.0  4.40    3.34    1.14  0.000054  0.000052  0.000013  \n",
       "27  0.0  3.69    3.34    1.14  0.000056  0.000052  0.000013  \n",
       "28  0.0  2.15    3.34    1.14  0.000034  0.000052  0.000013  \n",
       "29  0.0  4.10    3.34    1.14  0.000044  0.000052  0.000013  \n",
       "30  0.0  0.54    3.34    1.14  0.000041  0.000052  0.000013  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting full GLAM\n",
    "print('Fitting full GLAM hierarchically...')\n",
    "\n",
    "glam_full = glam.GLAM(train_data)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_FF2018_full_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_full.make_model('hierarchical', gamma_bounds=(-1, 1), t0_val=0)\n",
    "    glam_full.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_full.estimates = np.load(str('results/estimates/glam_FF2018_full_hierarchical_cv'+sufix+'.npy'))   \n",
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_FF2018_full_hierarchical_cv'+sufix+'.npy'), glam_full.estimates)\n",
    "pd.DataFrame(glam_full.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing WAIC scores for full model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Compute WAICs\n",
    "print('Computing WAIC scores for full model...')\n",
    "if not os.path.exists(str('results/waic/glam_FF2018_full'+ sufix +'.npy')):\n",
    "    # Note: DIC computation does not work for ADVI fitted models\n",
    "    # But we are using WAIC\n",
    "    glam_full.compute_waic()\n",
    "else:\n",
    "    print('  Found old DIC scores in \"results/waic\". Skipping WAIC computation...')\n",
    "    glam_full.waic = np.load(str('results/waic/glam_FF2018_full'+ sufix +'.npy'))\n",
    "\n",
    "# Compute WAICs\n",
    "np.save(str('results/waic/glam_FF2018_full'+ sufix +'.npy'), glam_full.waic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WAIC_r(WAIC=32704.32728053088, WAIC_se=0.0, p_WAIC=66.66452633644445, var_warn=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glam_full.waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/pymc3/stats.py:292: UserWarning: Estimated shape parameter of Pareto distribution is\n",
      "        greater than 0.7 for one or more samples.\n",
      "        You should consider using a more robust model, this is because\n",
      "        importance sampling is less likely to work well if the marginal\n",
      "        posterior and LOO posterior are very different. This is more likely to\n",
      "        happen with a non-robust model and highly influential observations.\n",
      "  happen with a non-robust model and highly influential observations.\"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_full.loo = pm.loo(trace=glam_full.trace, model=glam_full.model)\n",
    "glam_full.loo\n",
    "np.save(str('results/loo/glam_FF2018_full'+ sufix +'.npy'), glam_full.loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOO_r(LOO=32653.366046821335, LOO_se=0.0, p_LOO=41.183909481671435, shape_warn=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glam_full.loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test set data using full GLAM...\n",
      "Replaced attached data (1860 trials) with new data (1860 trials)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choice</th>\n",
       "      <th>repeat</th>\n",
       "      <th>rt</th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2779.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   choice  repeat      rt  subject  trial  item_value_0    gaze_0  \\\n",
       "0     0.0     0.0  3360.0      0.0    0.0           2.0  0.762332   \n",
       "1     0.0     1.0  1598.0      0.0    0.0           2.0  0.762332   \n",
       "2     0.0     2.0  2172.0      0.0    0.0           2.0  0.762332   \n",
       "3     0.0     3.0  2779.0      0.0    0.0           2.0  0.762332   \n",
       "4     0.0     4.0  1360.0      0.0    0.0           2.0  0.762332   \n",
       "\n",
       "   item_value_1    gaze_1  \n",
       "0           1.7  0.237668  \n",
       "1           1.7  0.237668  \n",
       "2           1.7  0.237668  \n",
       "3           1.7  0.237668  \n",
       "4           1.7  0.237668  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using full GLAM...')\n",
    "glam_full.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_FF2018_full_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_full.predict(n_repeats=50)\n",
    "    glam_full.prediction.to_csv(str('results/predictions/glam_FF2018_full_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical full GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_full.prediction = pd.read_csv(str('results/predictions/glam_FF2018_full_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_full.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. no-bias GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting no-bias GLAM hierarchically...\n",
      "Generating hierarchical model for 31 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 model(s) using NUTS...\n",
      "  Fitting model 1 of 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [tau, tau_sd, tau_mu, SNR, SNR_sd, SNR_mu, v, v_sd, v_mu]\n",
      "Sampling 2 chains:   0%|          | 0/6000 [00:00<?, ?draws/s]/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Sampling 2 chains: 100%|██████████| 6000/6000 [5:10:06<00:00,  2.14s/draws]\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/!\\ Automatically setting parameter precision...\n"
     ]
    }
   ],
   "source": [
    "# Fitting no-bias GLAM\n",
    "print('Fitting no-bias GLAM hierarchically...')\n",
    "\n",
    "glam_nobias = glam.GLAM(train_data)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_FF2018_nobias_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_nobias.make_model('hierarchical', gamma_val=1.0, t0_val=0)\n",
    "    glam_nobias.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_nobias.estimates = np.load(str('results/estimates/glam_FF2018_nobias_hierarchical_cv'+sufix+'.npy'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNR</th>\n",
       "      <th>SNR_mu</th>\n",
       "      <th>SNR_sd</th>\n",
       "      <th>b</th>\n",
       "      <th>gamma</th>\n",
       "      <th>p_error</th>\n",
       "      <th>s</th>\n",
       "      <th>t0</th>\n",
       "      <th>tau</th>\n",
       "      <th>tau_mu</th>\n",
       "      <th>tau_sd</th>\n",
       "      <th>v</th>\n",
       "      <th>v_mu</th>\n",
       "      <th>v_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177.66</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127.76</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155.13</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>413.19</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.02</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>496.86</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>105.38</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>129.34</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>136.84</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>140.92</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>146.59</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>180.92</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>313.61</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>146.47</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>101.96</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>135.76</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>129.82</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>97.52</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>146.95</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>48.93</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>174.33</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>186.06</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>185.44</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>187.38</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>218.98</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>148.95</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>127.81</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17.67</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>169.61</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>159.06</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>161.58</td>\n",
       "      <td>144.07</td>\n",
       "      <td>31.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SNR  SNR_mu  SNR_sd    b  gamma  p_error         s   t0   tau  tau_mu  \\\n",
       "0   177.66  144.07    31.4  1.0    1.0     0.05  0.007513  0.0  2.77    1.29   \n",
       "1   127.76  144.07    31.4  1.0    1.0     0.05  0.010208  0.0  2.07    1.29   \n",
       "2   155.13  144.07    31.4  1.0    1.0     0.05  0.009075  0.0  1.95    1.29   \n",
       "3   413.19  144.07    31.4  1.0    1.0     0.05  0.006931  0.0  1.80    1.29   \n",
       "4    91.02  144.07    31.4  1.0    1.0     0.05  0.007725  0.0  3.32    1.29   \n",
       "5   496.86  144.07    31.4  1.0    1.0     0.05  0.007489  0.0  1.32    1.29   \n",
       "6   105.38  144.07    31.4  1.0    1.0     0.05  0.009102  0.0  3.40    1.29   \n",
       "7   129.34  144.07    31.4  1.0    1.0     0.05  0.006906  0.0  1.28    1.29   \n",
       "8   136.84  144.07    31.4  1.0    1.0     0.05  0.008900  0.0  0.10    1.29   \n",
       "9   140.92  144.07    31.4  1.0    1.0     0.05  0.007791  0.0  3.19    1.29   \n",
       "10  146.59  144.07    31.4  1.0    1.0     0.05  0.007975  0.0  0.52    1.29   \n",
       "11  180.92  144.07    31.4  1.0    1.0     0.05  0.008096  0.0  2.65    1.29   \n",
       "12  313.61  144.07    31.4  1.0    1.0     0.05  0.008442  0.0  1.51    1.29   \n",
       "13  146.47  144.07    31.4  1.0    1.0     0.05  0.008012  0.0  1.80    1.29   \n",
       "14  101.96  144.07    31.4  1.0    1.0     0.05  0.005415  0.0  2.27    1.29   \n",
       "15  135.76  144.07    31.4  1.0    1.0     0.05  0.007838  0.0  0.89    1.29   \n",
       "16  129.82  144.07    31.4  1.0    1.0     0.05  0.007864  0.0  1.36    1.29   \n",
       "17   97.52  144.07    31.4  1.0    1.0     0.05  0.006869  0.0  0.32    1.29   \n",
       "18  146.95  144.07    31.4  1.0    1.0     0.05  0.008268  0.0  0.30    1.29   \n",
       "19   48.93  144.07    31.4  1.0    1.0     0.05  0.004935  0.0  0.34    1.29   \n",
       "20  174.33  144.07    31.4  1.0    1.0     0.05  0.007782  0.0  3.21    1.29   \n",
       "21  186.06  144.07    31.4  1.0    1.0     0.05  0.006724  0.0  0.21    1.29   \n",
       "22  185.44  144.07    31.4  1.0    1.0     0.05  0.009099  0.0  2.03    1.29   \n",
       "23  187.38  144.07    31.4  1.0    1.0     0.05  0.007851  0.0  0.51    1.29   \n",
       "24  218.98  144.07    31.4  1.0    1.0     0.05  0.007317  0.0  1.88    1.29   \n",
       "25  148.95  144.07    31.4  1.0    1.0     0.05  0.008325  0.0  1.39    1.29   \n",
       "26  127.81  144.07    31.4  1.0    1.0     0.05  0.007924  0.0  0.16    1.29   \n",
       "27   17.67  144.07    31.4  1.0    1.0     0.05  0.009511  0.0  0.29    1.29   \n",
       "28  169.61  144.07    31.4  1.0    1.0     0.05  0.006413  0.0  0.79    1.29   \n",
       "29  159.06  144.07    31.4  1.0    1.0     0.05  0.008503  0.0  0.45    1.29   \n",
       "30  161.58  144.07    31.4  1.0    1.0     0.05  0.007733  0.0  0.30    1.29   \n",
       "\n",
       "    tau_sd         v      v_mu      v_sd  \n",
       "0     0.76  0.000048  0.000058  0.000015  \n",
       "1     0.76  0.000039  0.000058  0.000015  \n",
       "2     0.76  0.000057  0.000058  0.000015  \n",
       "3     0.76  0.000050  0.000058  0.000015  \n",
       "4     0.76  0.000090  0.000058  0.000015  \n",
       "5     0.76  0.000032  0.000058  0.000015  \n",
       "6     0.76  0.000086  0.000058  0.000015  \n",
       "7     0.76  0.000048  0.000058  0.000015  \n",
       "8     0.76  0.000062  0.000058  0.000015  \n",
       "9     0.76  0.000044  0.000058  0.000015  \n",
       "10    0.76  0.000053  0.000058  0.000015  \n",
       "11    0.76  0.000056  0.000058  0.000015  \n",
       "12    0.76  0.000036  0.000058  0.000015  \n",
       "13    0.76  0.000047  0.000058  0.000015  \n",
       "14    0.76  0.000051  0.000058  0.000015  \n",
       "15    0.76  0.000057  0.000058  0.000015  \n",
       "16    0.76  0.000063  0.000058  0.000015  \n",
       "17    0.76  0.000070  0.000058  0.000015  \n",
       "18    0.76  0.000056  0.000058  0.000015  \n",
       "19    0.76  0.000106  0.000058  0.000015  \n",
       "20    0.76  0.000045  0.000058  0.000015  \n",
       "21    0.76  0.000036  0.000058  0.000015  \n",
       "22    0.76  0.000072  0.000058  0.000015  \n",
       "23    0.76  0.000043  0.000058  0.000015  \n",
       "24    0.76  0.000035  0.000058  0.000015  \n",
       "25    0.76  0.000048  0.000058  0.000015  \n",
       "26    0.76  0.000059  0.000058  0.000015  \n",
       "27    0.76  0.000059  0.000058  0.000015  \n",
       "28    0.76  0.000038  0.000058  0.000015  \n",
       "29    0.76  0.000055  0.000058  0.000015  \n",
       "30    0.76  0.000046  0.000058  0.000015  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \n",
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_FF2018_nobias_hierarchical_cv'+sufix+'.npy'), glam_nobias.estimates)\n",
    "pd.DataFrame(glam_nobias.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNR</th>\n",
       "      <th>SNR_mu</th>\n",
       "      <th>SNR_sd</th>\n",
       "      <th>b</th>\n",
       "      <th>gamma</th>\n",
       "      <th>p_error</th>\n",
       "      <th>s</th>\n",
       "      <th>t0</th>\n",
       "      <th>tau</th>\n",
       "      <th>tau_mu</th>\n",
       "      <th>tau_sd</th>\n",
       "      <th>v</th>\n",
       "      <th>v_mu</th>\n",
       "      <th>v_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168.63</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122.41</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151.55</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.06</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.26</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>112.54</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>215.37</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.011701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>202.01</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>161.54</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>167.26</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>145.86</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>146.76</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>142.95</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>153.58</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>142.90</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>156.47</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>153.80</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>116.37</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>95.62</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51.37</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>170.17</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>233.57</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>122.64</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>158.13</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>229.97</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>191.97</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>149.48</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>245.22</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>206.94</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>144.36</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>271.19</td>\n",
       "      <td>144.5</td>\n",
       "      <td>57.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SNR  SNR_mu  SNR_sd    b  gamma  p_error         s   t0   tau  tau_mu  \\\n",
       "0   168.63   144.5   57.82  1.0    1.0     0.05  0.007958  0.0  2.65    0.95   \n",
       "1   122.41   144.5   57.82  1.0    1.0     0.05  0.010110  0.0  1.78    0.95   \n",
       "2   151.55   144.5   57.82  1.0    1.0     0.05  0.008478  0.0  1.75    0.95   \n",
       "3   136.06   144.5   57.82  1.0    1.0     0.05  0.006783  0.0  1.11    0.95   \n",
       "4    79.26   144.5   57.82  1.0    1.0     0.05  0.007077  0.0  2.43    0.95   \n",
       "5   112.54   144.5   57.82  1.0    1.0     0.05  0.007223  0.0  1.58    0.95   \n",
       "6   215.37   144.5   57.82  1.0    1.0     0.05  0.011701  0.0  1.09    0.95   \n",
       "7   202.01   144.5   57.82  1.0    1.0     0.05  0.007848  0.0  1.25    0.95   \n",
       "8   161.54   144.5   57.82  1.0    1.0     0.05  0.009143  0.0  0.02    0.95   \n",
       "9   167.26   144.5   57.82  1.0    1.0     0.05  0.008773  0.0  1.47    0.95   \n",
       "10  145.86   144.5   57.82  1.0    1.0     0.05  0.006973  0.0  0.71    0.95   \n",
       "11  146.76   144.5   57.82  1.0    1.0     0.05  0.007493  0.0  0.41    0.95   \n",
       "12  142.95   144.5   57.82  1.0    1.0     0.05  0.007887  0.0  1.19    0.95   \n",
       "13  153.58   144.5   57.82  1.0    1.0     0.05  0.007044  0.0  1.39    0.95   \n",
       "14  142.90   144.5   57.82  1.0    1.0     0.05  0.006693  0.0  1.89    0.95   \n",
       "15  156.47   144.5   57.82  1.0    1.0     0.05  0.008742  0.0  1.54    0.95   \n",
       "16  153.80   144.5   57.82  1.0    1.0     0.05  0.008450  0.0  3.17    0.95   \n",
       "17  116.37   144.5   57.82  1.0    1.0     0.05  0.006639  0.0  1.39    0.95   \n",
       "18   95.62   144.5   57.82  1.0    1.0     0.05  0.006102  0.0  0.03    0.95   \n",
       "19   51.37   144.5   57.82  1.0    1.0     0.05  0.004361  0.0  0.16    0.95   \n",
       "20  170.17   144.5   57.82  1.0    1.0     0.05  0.008133  0.0  0.79    0.95   \n",
       "21  233.57   144.5   57.82  1.0    1.0     0.05  0.007873  0.0  0.96    0.95   \n",
       "22  122.64   144.5   57.82  1.0    1.0     0.05  0.009591  0.0  1.97    0.95   \n",
       "23  158.13   144.5   57.82  1.0    1.0     0.05  0.008661  0.0  0.68    0.95   \n",
       "24  229.97   144.5   57.82  1.0    1.0     0.05  0.007356  0.0  4.47    0.95   \n",
       "25  191.97   144.5   57.82  1.0    1.0     0.05  0.008214  0.0  1.24    0.95   \n",
       "26  149.48   144.5   57.82  1.0    1.0     0.05  0.009180  0.0  0.12    0.95   \n",
       "27  245.22   144.5   57.82  1.0    1.0     0.05  0.011570  0.0  0.93    0.95   \n",
       "28  206.94   144.5   57.82  1.0    1.0     0.05  0.007285  0.0  0.80    0.95   \n",
       "29  144.36   144.5   57.82  1.0    1.0     0.05  0.008070  0.0  0.66    0.95   \n",
       "30  271.19   144.5   57.82  1.0    1.0     0.05  0.007688  0.0  0.24    0.95   \n",
       "\n",
       "    tau_sd         v     v_mu      v_sd  \n",
       "0     0.94  0.000051  0.00006  0.000014  \n",
       "1     0.94  0.000082  0.00006  0.000014  \n",
       "2     0.94  0.000056  0.00006  0.000014  \n",
       "3     0.94  0.000050  0.00006  0.000014  \n",
       "4     0.94  0.000090  0.00006  0.000014  \n",
       "5     0.94  0.000064  0.00006  0.000014  \n",
       "6     0.94  0.000080  0.00006  0.000014  \n",
       "7     0.94  0.000046  0.00006  0.000014  \n",
       "8     0.94  0.000057  0.00006  0.000014  \n",
       "9     0.94  0.000050  0.00006  0.000014  \n",
       "10    0.94  0.000048  0.00006  0.000014  \n",
       "11    0.94  0.000051  0.00006  0.000014  \n",
       "12    0.94  0.000055  0.00006  0.000014  \n",
       "13    0.94  0.000046  0.00006  0.000014  \n",
       "14    0.94  0.000047  0.00006  0.000014  \n",
       "15    0.94  0.000056  0.00006  0.000014  \n",
       "16    0.94  0.000063  0.00006  0.000014  \n",
       "17    0.94  0.000057  0.00006  0.000014  \n",
       "18    0.94  0.000064  0.00006  0.000014  \n",
       "19    0.94  0.000101  0.00006  0.000014  \n",
       "20    0.94  0.000048  0.00006  0.000014  \n",
       "21    0.94  0.000034  0.00006  0.000014  \n",
       "22    0.94  0.000078  0.00006  0.000014  \n",
       "23    0.94  0.000054  0.00006  0.000014  \n",
       "24    0.94  0.000031  0.00006  0.000014  \n",
       "25    0.94  0.000057  0.00006  0.000014  \n",
       "26    0.94  0.000061  0.00006  0.000014  \n",
       "27    0.94  0.000059  0.00006  0.000014  \n",
       "28    0.94  0.000035  0.00006  0.000014  \n",
       "29    0.94  0.000053  0.00006  0.000014  \n",
       "30    0.94  0.000037  0.00006  0.000014  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case it is already fitted\n",
    "params_part_like = pd.DataFrame.from_dict(glam_nobias.estimates.item(0))\n",
    "params_part_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda/lib/python3.5/site-packages/pymc3/stats.py:292: UserWarning: Estimated shape parameter of Pareto distribution is\n",
      "        greater than 0.7 for one or more samples.\n",
      "        You should consider using a more robust model, this is because\n",
      "        importance sampling is less likely to work well if the marginal\n",
      "        posterior and LOO posterior are very different. This is more likely to\n",
      "        happen with a non-robust model and highly influential observations.\n",
      "  happen with a non-robust model and highly influential observations.\"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_nobias.loo = pm.loo(trace=glam_nobias.trace, model=glam_nobias.model)\n",
    "glam_nobias.loo\n",
    "\n",
    "np.save(str('results/loo/glam_FF2018_nobias'+ sufix +'.npy'), glam_nobias.loo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test set data using no-bias GLAM...\n",
      "Replaced attached data (1860 trials) with new data (1860 trials)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choice</th>\n",
       "      <th>repeat</th>\n",
       "      <th>rt</th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3377.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3097.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   choice  repeat      rt  subject  trial  item_value_0    gaze_0  \\\n",
       "0     0.0     0.0  4306.0      0.0    0.0           2.0  0.762332   \n",
       "1     0.0     1.0  1850.0      0.0    0.0           2.0  0.762332   \n",
       "2     0.0     2.0  2673.0      0.0    0.0           2.0  0.762332   \n",
       "3     0.0     3.0  3377.0      0.0    0.0           2.0  0.762332   \n",
       "4     0.0     4.0  3097.0      0.0    0.0           2.0  0.762332   \n",
       "\n",
       "   item_value_1    gaze_1  \n",
       "0           1.7  0.237668  \n",
       "1           1.7  0.237668  \n",
       "2           1.7  0.237668  \n",
       "3           1.7  0.237668  \n",
       "4           1.7  0.237668  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using no-bias GLAM...')\n",
    "glam_nobias.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_FF2018_nobias_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_nobias.predict(n_repeats=50)\n",
    "    glam_nobias.prediction.to_csv(str('results/predictions/glam_FF2018_nobias_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical no-bias GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_nobias.prediction = pd.read_csv(str('results/predictions/glam_FF2018_nobias_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_nobias.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close Figure to continue...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glam_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-aacf88876c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Close Figure to continue...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mglam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglam_nobias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glam_full' is not defined"
     ]
    }
   ],
   "source": [
    "print('Close Figure to continue...')\n",
    "glam.plot_fit(test_data, [glam_full.prediction]);\n",
    "glam.plot_fit(test_data, [glam_full.prediction,glam_nobias.prediction]);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for full hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_participant = glam_full.estimates.item(0)\n",
    "params_participant = pd.DataFrame.from_dict(glam_full.estimates.item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean gamma \" +  str(params_participant['gamma'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = params_participant[['SNR','gamma','tau','v']].hist(figsize = [20,3] , layout=[1,4],bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [END] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testa = glam_nobias.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlims =(0, 10)\n",
    "\n",
    "# Compute relevant variables\n",
    "df = glam.plots.add_difficulty(testa)\n",
    "\n",
    "# Compute summary statistics\n",
    "subject_means = df.groupby(['subject', 'difficulty']).rt.mean()\n",
    "means = subject_means.groupby('difficulty').mean()[xlims[0]:xlims[1]]\n",
    "sems = subject_means.groupby('difficulty').sem()[xlims[0]:xlims[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
